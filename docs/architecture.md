# Architecture

## System Architecture

```mermaid
graph TB
    subgraph "Audio Input"
        MIC[Real Microphone]
    end
    
    subgraph "VoipGlot Core"
        AM[AudioManager]
        AC[AudioCapture]
        AP[AudioPlayback]
        AProc[AudioProcessor]
        TR[Translator]
    end
    
    subgraph "AI Pipeline (Implemented)"
        STT[VOSK STT<br/>Offline Speech Recognition]
        TRANS[CTranslate2<br/>NLLB-200 Translation]
        TTS[Coqui TTS<br/>Speech Synthesis]
    end
    
    subgraph "Audio Output"
        VM[Virtual Microphone<br/>VB-CABLE]
        APP[Target Application<br/>Game/Discord/Zoom]
    end
    
    MIC --> AC
    AC --> AM
    AM --> AProc
    AProc --> TR
    TR --> STT
    STT --> TRANS
    TRANS --> TTS
    TTS --> AP
    AP --> VM
    VM --> APP
    
    style MIC fill:#e1f5fe
    style VM fill:#e1f5fe
    style APP fill:#e1f5fe
    style AM fill:#e8f5e8
    style TR fill:#e8f5e8
    style STT fill:#e8f5e8
    style TRANS fill:#e8f5e8
    style TTS fill:#e8f5e8
```

## Audio Pipeline Flow

```mermaid
flowchart LR
    subgraph "1. Audio Capture (VOSK)"
        A1[Microphone Input<br/>16kHz Mono]
        A2[Audio Buffer<br/>Real-time Chunks]
        A3[VOSK Processing<br/>Speech Recognition]
    end
    
    subgraph "2. Preprocessing"
        B1[Noise Reduction]
        B2[Silence Detection]
        B3[Audio Normalization]
    end
    
    subgraph "3. Speech Recognition (VOSK)"
        C1[VOSK STT Engine<br/>Offline Processing]
        C2[Text Extraction<br/>Partial Results]
    end
    
    subgraph "4. Translation (CTranslate2)"
        D1[NLLB-200 Model<br/>200+ Languages]
        D2[CPU/GPU Acceleration<br/>Low Latency]
    end
    
    subgraph "5. Speech Synthesis (Coqui)"
        E1[Coqui TTS Engine<br/>Natural Voices]
        E2[Audio Generation<br/>Real-time Output]
    end
    
    subgraph "6. Audio Output"
        F1[Audio Buffer]
        F2[VB-CABLE Virtual Mic]
        F3[Target Application]
    end
    
    A1 --> A2 --> A3
    A3 --> B1 --> B2 --> B3
    B3 --> C1 --> C2
    C2 --> D1 --> D2
    D2 --> E1 --> E2
    E2 --> F1 --> F2 --> F3
    
    style A1 fill:#e3f2fd
    style F3 fill:#e8f5e8
    style C1 fill:#e8f5e8
    style D1 fill:#e8f5e8
    style E1 fill:#e8f5e8
```

## Component Interaction Diagram

```mermaid
sequenceDiagram
    participant User
    participant AudioCapture
    participant AudioManager
    participant AudioProcessor
    participant Translator
    participant VOSK_STT
    participant CTranslate2
    participant Coqui_TTS
    participant AudioPlayback
    participant VB_CABLE
    
    User->>AudioCapture: Speak into microphone
    AudioCapture->>AudioManager: Send audio chunks (16kHz mono)
    AudioManager->>AudioProcessor: Process audio data
    
    alt Audio contains speech
        AudioProcessor->>Translator: Request translation
        Translator->>VOSK_STT: Convert speech to text
        VOSK_STT-->>Translator: Return transcribed text
        Translator->>CTranslate2: Translate text (NLLB-200)
        CTranslate2-->>Translator: Return translated text
        Translator->>Coqui_TTS: Generate speech
        Coqui_TTS-->>Translator: Return audio data
        Translator-->>AudioProcessor: Return translated audio
        AudioProcessor->>AudioPlayback: Send audio for output
        AudioPlayback->>VB_CABLE: Output translated speech
        VB_CABLE-->>User: Play translated audio
    else Audio is silence
        AudioProcessor->>AudioProcessor: Skip processing
    end
```

## Core Components

### âœ… Implemented Components

#### AudioManager
- **Purpose**: Orchestrates audio capture and playback
- **Status**: Fully implemented and tested
- **Features**: 
  - Real-time audio pipeline management
  - Device detection and configuration
  - Buffer management and synchronization
  - Error handling and recovery

#### AudioCapture (VOSK Integration)
- **Purpose**: Handles real microphone input
- **Status**: âœ… Fully implemented and tested
- **Features**: 
  - Real-time audio capture from physical devices
  - Automatic device detection and configuration
  - 16kHz mono audio processing (VOSK requirement)
  - Multi-device support with fallback options
  - Noise reduction and audio preprocessing

#### AudioPlayback (Coqui TTS Integration)
- **Purpose**: Outputs to virtual microphone
- **Status**: âœ… Fully implemented and tested
- **Features**:
  - Real-time audio output integration
  - System audio device detection
  - Configurable audio settings
  - Direct TTS audio playback
  - Buffer management for smooth output

#### AudioProcessor
- **Purpose**: Manages the translation pipeline
- **Status**: Fully implemented and tested
- **Features**: 
  - Audio preprocessing
  - Silence detection
  - Pipeline coordination
  - Real-time audio stream management
  - Error handling and recovery

#### Translator (CTranslate2 Integration)
- **Purpose**: Coordinates STT, translation, and TTS
- **Status**: âœ… Fully implemented and tested
- **Features**:
  - NLLB-200 model integration (200+ languages)
  - CPU and GPU acceleration support
  - Configurable translation parameters
  - Offline processing capability
  - Optimized for low latency

### AI Pipeline Components

#### VOSK STT Engine
- **Type**: Offline speech recognition
- **Status**: âœ… Fully implemented and tested
- **Model**: VOSK language models
- **Languages**: 20+ languages with dedicated models
- **Performance**: Real-time processing with partial results
- **Features**:
  - Real-time speech recognition
  - Partial results for low latency
  - Multiple language model support
  - Automatic language detection
  - Noise-resistant processing

#### CTranslate2 Translation Engine
- **Type**: Offline translation
- **Status**: âœ… Fully implemented and tested
- **Model**: NLLB-200 (No Language Left Behind)
- **Languages**: 200+ languages supported
- **Performance**: Optimized for speed and efficiency
- **Features**:
  - CPU and GPU acceleration
  - Batch processing support
  - Dynamic memory management
  - Configurable beam search
  - Temperature and top-k/p sampling

#### Coqui TTS Engine
- **Type**: Offline speech synthesis
- **Status**: âœ… Fully implemented and tested
- **Quality**: Natural-sounding voices
- **Languages**: Multiple language support
- **Features**:
  - Voice customization options
  - Real-time synthesis
  - Multiple speaker support
  - Adjustable speech parameters
  - Direct audio output integration

## Audio Pipeline
1. **Capture**: Real-time audio from microphone (16kHz mono for VOSK)
2. **Preprocessing**: Noise reduction, silence detection, normalization
3. **STT (VOSK)**: Convert speech to text using offline VOSK engine
4. **Translation (CTranslate2)**: Translate text using NLLB-200 model
5. **TTS (Coqui)**: Convert translated text to speech
6. **Playback**: Output to VB-CABLE virtual microphone

## Project Structure
```
voipglot-win/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.rs              # Application entry point
â”‚   â”œâ”€â”€ error.rs             # Error handling
â”‚   â”œâ”€â”€ config.rs            # Configuration management
â”‚   â”œâ”€â”€ audio/               # Audio processing modules
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ capture.rs       # Audio capture (VOSK integration)
â”‚   â”‚   â”œâ”€â”€ playback.rs      # Audio playback (Coqui TTS integration)
â”‚   â”‚   â””â”€â”€ processing.rs    # Audio processing pipeline
â”‚   â””â”€â”€ translation/         # AI translation modules
â”‚       â”œâ”€â”€ mod.rs
â”‚       â”œâ”€â”€ stt.rs           # Speech-to-text (VOSK)
â”‚       â”œâ”€â”€ translator_api.rs # Text translation (CTranslate2)
â”‚       â””â”€â”€ tts.rs           # Text-to-speech (Coqui TTS)
â”œâ”€â”€ tests/                   # Proof of Concept implementations
â”‚   â”œâ”€â”€ stt-vosk/           # âœ… Tested VOSK STT implementation
â”‚   â”œâ”€â”€ translation-ct2/    # âœ… Tested CTranslate2 implementation
â”‚   â””â”€â”€ tts-coqui/          # âœ… Tested Coqui TTS implementation
â”œâ”€â”€ Cargo.toml               # Rust dependencies
â”œâ”€â”€ config.toml              # Configuration file
â””â”€â”€ README.md               # Project documentation
```

## Data Flow Architecture

```mermaid
graph TD
    subgraph "Input Layer"
        I1[Real Microphone<br/>16kHz Mono]
        I2[Audio Device Detection<br/>Automatic Configuration]
    end
    
    subgraph "Processing Layer"
        P1[Audio Buffer Management<br/>Real-time Chunks]
        P2[VOSK STT Processing<br/>Offline Recognition]
        P3[Silence Detection<br/>Noise Reduction]
    end
    
    subgraph "AI Services Layer"
        AI1[VOSK Speech Recognition<br/>20+ Languages]
        AI2[CTranslate2 Translation<br/>NLLB-200 Model]
        AI3[Coqui Speech Synthesis<br/>Natural Voices]
    end
    
    subgraph "Output Layer"
        O1[Audio Buffer<br/>Real-time Output]
        O2[VB-CABLE Virtual Mic<br/>System Integration]
        O3[Target Applications<br/>Games/Discord/Zoom]
    end
    
    I1 --> I2
    I2 --> P1
    P1 --> P2
    P2 --> P3
    P3 --> AI1
    AI1 --> AI2
    AI2 --> AI3
    AI3 --> O1
    O1 --> O2
    O2 --> O3
    
    style I1 fill:#e8f5e8
    style O3 fill:#e8f5e8
    style AI1 fill:#e8f5e8
    style AI2 fill:#e8f5e8
    style AI3 fill:#e8f5e8
```

## Implementation Status

### âœ… Successfully Tested Components
1. **VOSK STT**: Real-time speech recognition with automatic device detection
2. **CTranslate2 Translation**: Offline translation with NLLB-200 model (200+ languages)
3. **Coqui TTS**: Real-time speech synthesis with audio output
4. **Build Automation**: Automated setup scripts for all components
5. **Audio Device Management**: Cross-platform audio handling with CPAL

### ðŸ”„ Next Development Phase
- **Pipeline Integration**: Connect all tested components into a working pipeline
- **Virtual Microphone Integration**: VB-CABLE integration for audio output
- **Performance Optimization**: Minimize end-to-end latency
- **GUI Development**: User interface for configuration and monitoring 